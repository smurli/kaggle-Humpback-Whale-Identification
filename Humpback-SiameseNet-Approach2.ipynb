{"cells":[{"metadata":{"_uuid":"df8e49f32864ea192ea7261975cd49a75271ee27"},"cell_type":"markdown","source":"# Inspired by \n* https://sorenbouma.github.io/blog/oneshot/\n* https://www.kaggle.com/udaygurugubelli/one-shot-siamese-networks/data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.regularizers import l2\nfrom keras import backend as K\nfrom keras.optimizers import SGD,Adam\nfrom keras.losses import binary_crossentropy\nimport numpy.random as rng\nimport numpy as np\nimport os\nimport dill as pickle\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31091034388804ecf66421fafde5af231cea5738"},"cell_type":"code","source":"# Initialize weights and bias as per paper\ndef W_init(shape,name=None):\n    \"\"\"Initialize weights as in paper\"\"\"\n    values = rng.normal(loc=0,scale=1e-2,size=shape)\n    return K.variable(values,name=name)\n\ndef b_init(shape,name=None):\n    \"\"\"Initialize bias as in paper\"\"\"\n    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n    return K.variable(values,name=name)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Build siamese Network"},{"metadata":{"trusted":true,"_uuid":"1848afea80602a4842c880831a68428bae0607df"},"cell_type":"code","source":"input_shape = (105, 105, 1)\nleft_input = Input(input_shape)\nright_input = Input(input_shape)\n#build convnet to use in each siamese 'leg'\nconvnet = Sequential()\nconvnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\nconvnet.add(MaxPooling2D())\nconvnet.add(Conv2D(128,(7,7),activation='relu',\n                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\nconvnet.add(MaxPooling2D())\nconvnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\nconvnet.add(MaxPooling2D())\nconvnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\nconvnet.add(Flatten())\nconvnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n#encode each of the two inputs into a vector with the convnet\nencoded_l = convnet(left_input)\nencoded_r = convnet(right_input)\n#merge two encoded inputs with the l1 distance between them\nL1_distance = lambda x: K.abs(x[0]-x[1])\n#both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\n#merge function is depreciated, using substract to find diff between layers output\nboth = merge.subtract([encoded_l,encoded_r])\nprediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\nsiamese_net = Model(input=[left_input,right_input],output=prediction)\n#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n\noptimizer = Adam(0.00006)\n#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\nsiamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n\nsiamese_net.count_params()\nprint(\"Done!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}